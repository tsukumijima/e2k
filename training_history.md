# E2K モデル学習の経緯と最適化プロセス

## 概要

本ドキュメントは、`e2k` プロジェクトにおける英語→カタカナ変換モデル（C2K / P2K）の学習最適化プロセスを詳細に記録したものです。  
オリジナルのデータセット（約3万件）から、より大規模で高品質なデータセット（約15万件）への移行に伴う、ハイパーパラメータ調整とアーキテクチャ改善の試行錯誤をまとめています。

## 最終成果

### C2K (Character-to-Katakana) モデル
- **BLEU スコア**: 0.9626（オリジナルデータセットでの評価: 0.9209）
- **モデルファイル**: `vendor/model-c2k-best_11241800.pth`
- **モデル次元数**: 256（推論速度を優先した軽量設計）

### P2K (Phoneme-to-Katakana) モデル
- **BLEU スコア**: 0.8511（オリジナルデータセットでの評価: 0.6890）
- **モデルファイル**: `vendor/model-p2k-best_11242004.pth`
- **モデル次元数**: 256

## データセットの背景

### オリジナルデータセット
- **規模**: 約3万件
- **出典**: Wiktionary と JMdict/EDICT から抽出
- **特徴**: 表記揺れやノイズ（例：「die」→「ポア」のような誤った読み）が含まれる
- **評価**: C2K で BLEU 0.92 程度が限界

### 新データセット（`katakana_dict.jsonl`）
- **規模**: 約15万件（5倍）
- **出典**: 複数の高品質なデータソースをマージ
- **特徴**: 
  - 徹底的なクレンジング処理
  - データソースの信頼度に基づく優先順位付け
  - 表記の整合性が極めて高い
- **課題**: データが高品質すぎるため、オリジナルのノイズデータで学習したモデルではスコアが下がる（0.92 → 0.69 など）

## 学習プロセスの変遷

### Phase 1: 初期の試行錯誤（Batch Size と Scheduler の調整）

**問題**: オリジナルの設定（Batch Size 64, ExponentialLR）をそのまま使うと、大規模データセットで学習が不安定になる可能性があると判断し、モダンな手法を導入。

**実施した変更**:
- Batch Size: 64 → 256 → 512（GPU リソースに余裕があったため）
- Scheduler: ExponentialLR → ReduceLROnPlateau（停滞を検知してから学習率を下げる）
- Optimizer: Adam → AdamW（Weight Decay の正しい実装）
- Dropout: 0.0 → 0.1（過学習防止）

**結果**: 
- Loss が 1.0 付近から下がらない（Label Smoothing の影響もあった）
- BLEU スコア: 0.9091（オリジナルの 0.92 に届かない）

**分析**: 
- バッチサイズを大きくしすぎたことで、更新回数が減り、モデルが「無難な解」に安住してしまった
- ReduceLROnPlateau は「停滞するまで待つ」ため、小規模モデル（dim=256）では最適解の周りをウロウロしてしまう
- Label Smoothing は「正解を 100% 信じない」手法のため、高品質なデータセットに対しては逆効果

### Phase 2: オリジナル設定への回帰とモダンな改善の統合

**問題**: Phase 1 の結果から、オリジナルの「攻撃的な学習設定」が、このタスクとモデルサイズには最適だったことが判明。

**実施した変更**:
- Batch Size: 512 → 256 → **64**（オリジナルに回帰）
- Scheduler: ReduceLROnPlateau → **ExponentialLR (gamma=0.9)**（強制的に収束させる）
- Optimizer: AdamW → **AdamW (weight_decay=0)**（重み減衰を無効化）
- Dropout: 0.1 → **0.0**（モデルの全容量を解放）
- Label Smoothing: 0.1 → **0.0**（標準の CrossEntropyLoss に戻す）
- Epochs: 100 → **50**（ExponentialLR で十分）

**追加した改善**:
- **重み初期化**: Xavier Initialization（Linear 層）と Orthogonal Initialization（GRU 層）を明示的に実装
  - これにより学習初期の不安定さを解消し、より深いところまでスムーズに学習が進むようになった

**結果**:
- C2K: BLEU 0.9626（大幅向上）
- P2K: BLEU 0.8511（オリジナルの 0.69 から大幅向上）
- Loss は適切に減少し、Early Stopping が機能（Epoch 9 で Best Model 保存）

## 最終的な学習設定

### ハイパーパラメータ

```python
DIM = 256                    # モデル次元数（推論速度優先）
BATCH_SIZE = 64              # 小さいバッチで適度なノイズを注入
LEARNING_RATE = 1e-3         # 初期学習率
EPOCHS = 50                  # 最大エポック数
DROPOUT = 0.0                # ドロップアウト無効（容量最大化）
WEIGHT_DECAY = 0             # 重み減衰無効（記憶力最大化）
SCHEDULER = ExponentialLR(gamma=0.9)  # 強制的に収束させる
EARLY_STOPPING_PATIENCE = 10 # 10エポック改善なしで停止
GRADIENT_CLIPPING = 1.0      # 勾配クリッピング
```

### アーキテクチャ

- **Encoder**: Bidirectional GRU (256次元)
- **Decoder**: 2層の GRU（Pre-decoder + Post-decoder）
- **Attention**: MultiheadAttention (4 heads)
- **重み初期化**: 
  - Linear 層: Xavier Uniform
  - GRU 層: Orthogonal（weight_hh）, Xavier Uniform（weight_ih）

### 学習の挙動

**C2K モデル**:
- Epoch 1-9: Val Loss が順調に減少（0.3965 → 0.2729）
- Epoch 9: Best Model 保存（Val Loss: 0.2729, BLEU: 0.8567）
- Epoch 10-19: Val Loss が上昇に転じる（過学習の兆候）
- Epoch 19: Early Stopping 発動
- **最終評価**: BLEU 0.9626（学習時の BLEU 0.8567 より大幅に高い）

**P2K モデル**:
- Epoch 1-6: Val Loss が順調に減少（0.6404 → 0.5292）
- Epoch 6: Best Model 保存（Val Loss: 0.5292, BLEU: 0.7579）
- Epoch 7-16: Val Loss が上昇に転じる
- Epoch 16: Early Stopping 発動
- **最終評価**: BLEU 0.8511

## 重要な発見と学び

### 1. 「小さいバッチサイズの魔法」
- Batch Size 64 は「ノイズ」を生むが、これが正則化として働き、鋭い極小解（Overfitting）から脱出させる
- 特に `dim=256` のような小規模モデルでは、バッチサイズを大きくしすぎると「無難な解」に安住してしまう

### 2. 「強制冷却」の重要性
- ExponentialLR は「時間経過で強制的に学習率を下げる」ため、モデルを物理的に収束（結晶化）させる
- ReduceLROnPlateau は「停滞するまで待つ」ため、小規模モデルでは最適解の周りをウロウロしてしまう

### 3. 「容量最大化」の必要性
- `dim=256` は 15万件のデータを覚えるには容量がカツカツ
- Dropout や Weight Decay などの「制限」を外すことで、モデルの全能力を解放する必要がある

### 4. 「Loss と BLEU の乖離」
- Loss (CrossEntropy) は「正解の文字に対する確率」を厳密に見る
- BLEU は「出てきた文字が合っているか」だけを見る
- 「自信満々ではない（Loss は残る）が、答えは合っている（BLEU は高い）」状態は健全

### 5. 「データセットの品質が全て」
- 高品質なデータセット（表記の整合性が高い）を用意することで、モデルは効率的にルールを獲得できる
- オリジナルのノイズデータで学習したモデルは、新しい高品質データではスコアが下がる（データの方言の違い）

## 推論速度とのトレードオフ

### モデルサイズの選択
- **256次元**: 推論速度を最優先（i5-9400 で約 150 inf/s）
- **512次元**: 精度を優先（推論速度は約 1/4 に低下）

### 推論実装
- PyTorch モデルを NumPy にエクスポート（`export.py`）
- `src/e2k/inference.py` で NumPy 実装による高速推論
- カスタムモデルファイルの読み込みに対応（`model_path` 引数）

## 今後の展望

### さらなる改善の余地
現状の設定（Batch 64, No Dropout, ExponentialLR）で、**「256次元モデル × RNN (GRU)」というアーキテクチャの限界値**に達しています。これ以上の改善（例：0.96 → 0.98）を目指すなら：

1. **アーキテクチャの変更**: Transformer 化など
2. **次元数の拡大**: 256 → 512 など
3. **推論時の改善**: Beam Search の導入（Greedy Search から脱却）

ただし、これらは「超高速推論」という要件とトレードオフになります。

### Beam Search の可能性
推論時に Beam Search（上位 K 個の候補を保持）を導入することで、BLEU スコアを 1-2 ポイント確実に上げることができます。ただし、推論速度は若干低下します（`K=2` 程度なら誤差の範囲）。

## ファイル構成

### 学習済みモデル
- `vendor/model-c2k-best_11241800.pth`: C2K の最良モデル（PyTorch）
- `vendor/model-c2k-best_11241800.npz`: C2K の最良モデル（NumPy エクスポート）
- `vendor/model-p2k-best_11242004.pth`: P2K の最良モデル（PyTorch）
- `vendor/model-p2k-best_11242004.npz`: P2K の最良モデル（NumPy エクスポート）

### データセット
- `vendor/katakana_dict.jsonl`: 新データセット（約15万件）
- `vendor/katakana_dict_original.jsonl`: オリジナルデータセット（約3万件）

### スクリプト
- `train.py`: 学習スクリプト（最適化済みのデフォルト値）
- `eval.py`: 評価スクリプト（自動次元検出機能付き）
- `export.py`: NumPy エクスポートスクリプト（自動次元検出機能付き）
- `benchmark_inference.py`: 推論速度ベンチマーク

## 実行コマンド

### 学習
```bash
# C2K モデル
uv run --no-sync python train.py --data vendor/katakana_dict.jsonl

# P2K モデル
uv run --no-sync python train.py --data vendor/katakana_dict.jsonl --p2k
```

### 評価
```bash
# C2K モデル
uv run --no-sync python eval.py --data vendor/katakana_dict.jsonl --model vendor/model-c2k-best_11241800.pth

# P2K モデル
uv run --no-sync python eval.py --data vendor/katakana_dict.jsonl --model vendor/model-p2k-best_11242004.pth --p2k
```

### エクスポート
```bash
# C2K モデル
uv run --no-sync python export.py --model vendor/model-c2k-best_11241800.pth --output vendor/model-c2k-best_11241800.npz

# P2K モデル
uv run --no-sync python export.py --model vendor/model-p2k-best_11242004.pth --output vendor/model-p2k-best_11242004.npz --p2k
```

## まとめ

本プロジェクトでは、オリジナルの「攻撃的な学習設定」をベースに、モダンな改善（重み初期化）を統合することで、**「超軽量（256次元）× 超高速 × 高品質データ完全対応」** のモデルを実現しました。

特に重要なのは、**「データセットの品質が全て」** という点です。高品質なデータセットを用意することで、小さなモデルでも高い精度を達成できることが実証されました。

現在の設定は、このアーキテクチャとデータセットの組み合わせにおける「最適解」であり、さらなるパラメータ調整は「誤差」の範囲に収まる可能性が高いです。

## 英語/ローマ字混合学習の実験

### 背景と課題

英語→カタカナ変換に加え、**ローマ字→カタカナ変換**（例：「nakahara」→「ナカハラ」）も単一モデルで処理したいという要件が発生しました。  
推論時にモードパラメータを指定するのではなく、入力の特徴から自動的に適切な変換ルールを選択するモデルを目指しました。

**データセットの構成**:
- 英語: 152,501件（98%）
- ローマ字: 3,130件（2%）

**2つのタスクの本質的な違い**:

| 観点 | 英語→カタカナ | ローマ字→カタカナ |
|------|---------------|-------------------|
| 変換ルール | 音韻的近似（複雑） | 直接的音節対応（シンプル） |
| パターン | 多様、例外多い | 一貫性が高い |
| 例 | "cat" → キャット | "nakahara" → ナカハラ |

### 初期の問題: 単純混合による性能低下

ローマ字データを単純にデータセットに追加して学習すると、**英語 BLEU が 0.96 → 0.93 程度に低下**する問題が発生しました。

**原因の分析**:
1. **データ不均衡**: 98% が英語データのため、各バッチの勾配のほとんどが英語タスク由来。ローマ字は「ノイズ」として扱われやすい
2. **タスク干渉**: 英語の複雑なパターンがローマ字のシンプルなルールを「上書き」してしまう
3. **容量の圧迫**: DIM=256 では 15万件のデータを覚えるのがカツカツ。タスクを追加するとさらに逼迫

### 解決策: 2段階学習（Phased Training）

**基本アイデア**:
- **Phase 1**: 英語データのみで学習し、音韻変換の基盤を確立
- **Phase 2**: ローマ字データを徐々に混ぜて追加学習

**実装した機能**:
1. `PhasedSampler`: Phase 1 ではローマ字を除外、Phase 2 では徐々にローマ字を混入するサンプラー
2. `evaluate_by_task()`: 英語 BLEU とローマ字 BLEU を分離して評価する関数
3. コマンドライン引数: `--phase1-epochs`, `--romaji-ratio`, `--rampup-epochs`

```python
# 推奨設定
--phase1-epochs 10    # Phase 1（英語のみ）のエポック数
--romaji-ratio 0.2    # Phase 2 での最終ローマ字比率
--rampup-epochs 10    # ローマ字比率が最終値に達するまでのエポック数
```

### 予想外の発見: Phase 1 だけで十分だった

実際に学習を実行したところ、**Phase 2 に入る前の Epoch 8 で Early Stopping が発動**し、驚くべき結果が得られました。

**学習結果**:
| メトリクス | 結果 | 比較 |
|------------|------|------|
| 英語 BLEU | **0.9714** | ↑ 従来の 0.9626 を上回る |
| ローマ字 BLEU | **0.9968** | ほぼ完璧 |
| 全体 BLEU | **0.9718** | - |
| Best Model 保存 | Epoch 8 | Phase 1 の終了（Epoch 10）より前 |

### メカニズムの解明: 「Train on one, validate on both」

Phase 2 に入らなかったにも関わらず、ローマ字 BLEU が 0.9968 という高スコアを達成した理由を分析しました。

**核心的なメカニズム**:

```
Training Data (Phase 1):    英語のみ
Validation Data (常時):      英語 + ローマ字
```

Phase 1 では学習データからローマ字を除外しますが、**検証データ（Val Loss 計算用）にはローマ字が含まれています**。これが以下のような効果を生みました。

1. **暗黙的な正則化**: 検証データにローマ字が含まれることで、「英語に過学習しすぎるとローマ字の Val Loss が上昇する」という制約が自然に発生

2. **早期停止の精度向上**: 従来（英語のみ）では Epoch 9 で停止していたが、ローマ字を含む検証により Epoch 8 で停止。「英語タスクへの過学習がローマ字に悪影響を与え始める直前」でモデルが保存された

3. **汎化性能の向上**: 英語に特化しすぎない「ほどよい汎化」状態でモデルが凍結されたため、ローマ字にも対応できる柔軟性を維持

**なぜローマ字が学習なしで高精度なのか**:
- ローマ字→カタカナ変換は「文字→音節」の直接対応であり、英語→カタカナで学習した「文字列→カタカナ」のパターンの**部分集合**として自然に獲得される
- 例: 英語で "ka" → "カ" を学習していれば、ローマ字の "ka" → "カ" も自動的に正解できる
- ローマ字特有の長音（"ou" → "オウ" ではなく "オー"）も、英語の類似パターンから転移学習される

### 重要な学び

#### 6. 「Train on one, validate on both」の威力
- 学習データと検証データの構成を意図的に変えることで、**明示的な Phase 2 なしに**マルチタスク学習が実現できる
- 検証データに「将来学習したいタスク」を含めることで、暗黙的な正則化が働く
- Early Stopping の判定基準を多様化することで、より汎化性能の高いモデルが得られる

#### 7. 「サブセット関係」にあるタスクの共同学習
- ローマ字→カタカナは、英語→カタカナの「簡単なサブセット」
- サブセット関係にあるタスクは、主タスクの学習だけで自動的に解決される可能性がある
- 検証データに含めることで、「過学習の早期検知器」として機能する

### 新しいデフォルト設定

`train.py` のデフォルト値は以下のように設定しました。

```python
--phase1-epochs 10    # デフォルト: 10（実質的に常に Phase 1）
--romaji-ratio 0.2    # Phase 2 に入った場合の最終比率
--rampup-epochs 10    # Phase 2 でのローマ字比率増加期間
```

**注意**: 実際の学習では Early Stopping により Phase 1 の途中（Epoch 8 前後）で停止することが多いため、Phase 2 の設定はほとんど使われません。  
しかし、より長いエポック数で学習する場合やデータセットを変更した場合に備えて、Phase 2 の機能は維持しています。

### タスク別評価の実行

学習後のモデルを評価する際は、`--by-task` オプションでタスク別の BLEU を確認できます。

```bash
uv run --no-sync python eval.py \
    --data vendor/katakana_dict.jsonl \
    --model vendor/model-c2k-best_12030549.pth \
    --by-task
```

出力例:
```
============================================================
Overall BLEU: 0.9718
English BLEU: 0.9714 (14972 samples)
Romaji BLEU:  0.9968 (313 samples)
============================================================
```
